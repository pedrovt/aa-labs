{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Lab 12 - LSTM for Air Polution Forecasting\n",
    "\n",
    "\n",
    "\n",
    "**Objectives**: Apply Long Short-Term Memory (LSTM) Recurrent Neural Network (RNN) for \n",
    "Multivariate Time Series Forecasting. \n",
    "\n",
    "**The forecasting problem**: Air Quality dataset reports on the weather and the level of pollution each hour over 5 years at the US embassy in Beijing, China. Given the weather conditions and pollution for prior hours, forecast the pollution at the next hour.\n",
    "\n",
    "Data includes the date-time, the pollution called PM2.5 concentration, and the weather information including dew point, temperature, pressure, wind direction, wind speed and the cumulative number of hours of snow and rain. The complete list of data is as follows:\n",
    "\n",
    "**Total data**: 43800 samples (5 years x 365 days x 24 hours), 8 features.\n",
    "\n",
    "**year-month-day-hour**: date of data in this row\n",
    "\n",
    "**pm2.5**: PM2.5 concentration  (**pollution indicator => the predicted variable**) \n",
    "\n",
    "**DEWP**: Dew Point (DewP indicates the amount moisture in the air. The higher the dewP, the higher the moisture content of the air at a given temperature.\n",
    "\n",
    "**TEM**P: Temperature\n",
    "\n",
    "**PRES**: Pressure\n",
    "\n",
    "**cbwd**: Combined wind direction (categorical feature, 4 values, e.g. Sought-East (SE))\n",
    "\n",
    "**Iws**: Cumulated wind speed\n",
    "\n",
    "**Is**: Cumulated hours of snow\n",
    "\n",
    "**Ir**: Cumulated hours of rain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "warnings.filterwarnings('ignore',category=RuntimeWarning)\n",
    "\n",
    "import numpy as np\n",
    "from numpy import concatenate\n",
    "from matplotlib import pyplot\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "Use function *read_csv* to load data from file pollution.csv and create the structure dataset. \n",
    "Then, extract only the values of the structure and ignore the column with the date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset =  ?\n",
    "\n",
    "values = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Data\n",
    "Plot the data and get a figure similar to Fig.1 \n",
    "<img src=\"images/f1.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> **Fig. 1** : ** Pollution & Weather Time series dataset** </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Data Preparation\n",
    "Prepare the dataset for the LSTM, that is, frame the dataset as a supervised learning problem to predict the pollution (pm2.5) at the current or future hours given the past pollution and weather conditions over a number of prior time steps (hours).\n",
    "\n",
    "\n",
    "**Original data frame (each row has data of only that hour)** :\n",
    "\n",
    "row 1=> var1(t), var2(t),...var8(t)\n",
    "\n",
    "row 2=> var1(t-1), var2(t-1),...var8(t)\n",
    "\n",
    "row 3=> .....\n",
    "\n",
    "**After series_to_supervised (each row has data of n previous hours and the current hour)**:\n",
    "\n",
    "\n",
    "row 1=> var1(t-n), var2(t-n),...var8(t-n), var1(t-n+1), var2(t-n+1),...var8(t-n+1), var1(t), var3(t),...var8(t)\n",
    "\n",
    "row 2=> var1(t-n-1), var2(t-n-1),...var8(t-n-1), var1(t-1), var3(t-1),...var8(t-1)\n",
    "\n",
    "row 3 => ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert series to supervised learning\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True): \n",
    "    \"\"\"\n",
    "        data: data matrix\n",
    "        n_in: number of input lag timesteps (hours)\n",
    "        n_out: number of output prediction timesteps (hours)\n",
    " \n",
    "    \"\"\"\n",
    "        \n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = DataFrame(data)  #transforms data (pure matrix of values) into data structure adding index of rows and columns\n",
    "    cols, names = list(), list()  #inicialize empty lists \n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the categorical feature cbwd: Combined wind direction (4 categorical values) into integers (0,1,2,3)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "values[:,4] = encoder.fit_transform(values[:,4]) # transform the 5th column into integers 0,1,2,3\n",
    "\n",
    "# ensure all data is float\n",
    "values = values.astype('float32')\n",
    "\n",
    "# normalize features\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled =  ?  #apply scaler to the values \n",
    "\n",
    "\n",
    "# specify the number of lag hours. You can change this hyper-parameter\n",
    "n_hours = 3\n",
    "\n",
    "# call series_to_supervised to frame data as supervised learning\n",
    "reframed = ?\n",
    "\n",
    "print(reframed.head(3))\n",
    "\n",
    "# we want to predict only the feature pm2.5 =var1(t)\n",
    "# Take out the columns of var2(t), var3(t),...var8(t). The indexes below are correct if n_hours = 3\n",
    "reframed.drop(reframed.columns[[31,30, 29, 28, 27, 26, 25]], axis=1, inplace=True)\n",
    "\n",
    "#Observe the difference before and after the drop step \n",
    "print(reframed.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Data\n",
    "\n",
    "First, data is split into train and test subsets. To speed up the model fiting, the training data is only the \n",
    "1st year, the test data are the remaining 4 years. You may consider exploring the oposite. \n",
    "\n",
    "Then the train and test sets are split into input and output variables. \n",
    "\n",
    "Finally, the inputs (X) are reshaped into 3D format expected by LSTMs, namely [samples, timesteps, features].\n",
    "\n",
    "**Total data**: 43800 samples (5 years x 365 days x 24 hours), 8 features.\n",
    "\n",
    "**Train data**: 3D tensor(samples, timesteps, features).\n",
    "8760 samples:  Data from 1 year (365 days), 24 times per day (1h period).\n",
    "\n",
    "**Test data**: 3D tensor (samples, timesteps, features). The rest of 35037 samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "\n",
    "# reframed is a structure that has not only values but also heads and row indexes. Extract only the values \n",
    "values = reframed.values\n",
    "n_train_hours = 365 * 24  #8760 (1 year, 24 times per day, i.e. period 1h)\n",
    "\n",
    "#Extract training subset from values \n",
    "train = ?\n",
    "\n",
    "#Extract test subset from values \n",
    "test = ?\n",
    "\n",
    "# split into input and outputs\n",
    "\n",
    "n_features =  ?  #number of features \n",
    "\n",
    "n_obs = n_hours * n_features  # 3*8 =24\n",
    "\n",
    "train_X  = ?   # train_X.shape = (8760, 24)\n",
    "train_y = ?  # train_y.shape = (8760,)\n",
    "\n",
    "test_X  =  ?      # test_X.shape = (35037, 24)\n",
    "test_y =  ? # test_y.shape = (35037,)\n",
    "\n",
    "# reshape input to be 3D [samples, lag_hours, features]\n",
    "train_X = ? # train_X.shape=(8760,3,8)\n",
    "test_X =  ?  # test_X.shape=(35037,3,8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define and Fit Model\n",
    "\n",
    "In this section, we define and fit an LSTM on the multivariate input data.\n",
    "We define the LSTM with 50 neurons in the first hidden layer and 1 neuron in the output layer for predicting pollution.\n",
    "We use the Mean Absolute Error (MAE) loss function and the Adam version of stochastic gradient descent.\n",
    "The model will be fit for 50 training epochs with a batch size of 72. \n",
    "At the end of the run both the training and test loss are plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential model is appropriate to design LSTM RNN\n",
    "model = Sequential()\n",
    "# train_X : 3D tensor (samples, lag_hours, feature)\n",
    "# neurons =50\n",
    "# single input_shape = (lag_hours, feature)\n",
    "model.add(LSTM(50, input_shape=(?)   ))\n",
    "model.add(Dense(1))  # 1 neuron in the output layer\n",
    "model.compile(loss='mae', optimizer='adam')\n",
    "\n",
    "# fit network\n",
    "#verbose=0 will show nothing (silent)\n",
    "#verbose=1 will show an animated progress bar\n",
    "#verbose=2 will show the training progress for each epoch (shows loss and val_loss)\n",
    "\n",
    "history = model.fit(train_X, train_y, epochs=50, batch_size=72, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
    "\n",
    "#  print history\n",
    "pyplot.plot(history.history['loss'])\n",
    "pyplot.plot(history.history['val_loss'])\n",
    "pyplot.title('Loss function')\n",
    "pyplot.legend(['train', 'test'])\n",
    "pyplot.xlabel('iterations')\n",
    "pyplot.show()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "After the model is fit, we can forecast for the test dataset.\n",
    "Compute the error as the Root Mean Squared Error (RMSE). \n",
    "\n",
    "Compare the predictions and the real data as shown in Fig. 2. \n",
    "\n",
    "<img src=\"images/f2.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> **Fig. 1** : ** Test data prediction (the first 2000 samples)** </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model.predict(test_X) \n",
    "\n",
    "# calculate RMSE\n",
    "rmse = np.sqrt(mean_squared_error(test_y, yhat))\n",
    "print('Test RMSE: %.3f' % rmse)     \n",
    "\n",
    "#Plot test_y and yhat on the same plot\n",
    "#use pyplot from matplotlib\n",
    "\n",
    "?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
