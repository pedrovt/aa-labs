{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Lab 3 - Logistic Regression\n",
    "\n",
    "## PART 2: Regularized Logistic Regression ##\n",
    "\n",
    "**Objectives**: Implement Regularized Logistic Regression and get to see it works on data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization shrinks model parameters $\\theta$ towards zero to prevent overfitting by reducing the variance of the model.\n",
    "\n",
    "**Problem**: You will implement regularized logistic regression to predict whether microchips from a fabrication plant pass quality assurance (QA). During QA, each microchip goes through various tests to ensure it is functioning correctly. Suppose you are the product manager of the factory and you have the test results for some microchips on two different tests. From these two tests, you would like to determine if the microchips should be accepted or rejected. To help you make the decision, you have a dataset of test results on past microchips, from which you can build a logistic regression model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data¶\n",
    "File *ex2data2.txt* contains the dataset for this problem.\n",
    "The first two columns are the tests, the 3rd column indicates if the microchips should be accepted(1) or rejected(0).\n",
    "\n",
    "Load data into the variable data (e.g. using function pd.read_csv from panda library) and then extract X (the features) and y (the labels).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(?,?)\n",
    "data_n=data.values #extract only the matrix 100x3 of values \n",
    "X= ?\n",
    "y=?\n",
    "\n",
    "##a few examples from the dataset \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data\n",
    "\n",
    "Create a scatter plot of data similar to Fig.1 (using plt.scatter).\n",
    "\n",
    "<img src=\"images/f3.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> **Fig. 1** : **file ex2data2.txt** </center></caption>\n",
    "\n",
    "The axes are the two test scores, and the positive (y = 1, accepted) and negative (y = 0, rejected) examples are shown with different markers. \n",
    "\n",
    "Plotting the data clearly shows that the decision boundary that separates the different classes is a non-linear one. \n",
    "Therefore, a straightforward application of logistic regression will not perform well on this dataset since logistic regression will only be able to find a linear decision boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Fig.1. The code is similar to the one in Part 1 of this lab. \n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Mapping\n",
    "\n",
    "One way to try to fit the data better is to create more features from the existing ones. \n",
    "In function *mapFeature*, we will map the features into polynomial terms of x1 and x2 up to the 6th power.\n",
    "\n",
    "As a result of this mapping, the vector of two features (the scores on two QA tests) has been transformed into a 28-dimensional vector (Fig.2). A logistic regression classifier trained on this higher-dimensional feature vector will have a more complex decision boundary and will appear nonlinear when drawn in the original 2-dimensional plot. While the feature mapping allows us to build a better classifier, it is also more susceptible to overfitting. Now, you will implement regularized logistic regression to fit the data and see how regularization can deal with the overfitting problem.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/f4.png\" style=\"width:250px;height:200px;\">\n",
    "<caption><center> **Fig. 2** : **Polynomial features** </center></caption>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapFeature(x1,x2,degree):\n",
    "    \"\"\"\n",
    "    take in numpy array of x1 and x2, return polynomial terms up to the given degree\n",
    "    \"\"\"\n",
    "    out = np.ones((len(x1),1))\n",
    "    for i in range(1,degree+1):\n",
    "        for j in range(i+1):\n",
    "            terms= (x1**(i-j) * x2**j).reshape(len(x1),1)\n",
    "            out= np.hstack((out,terms))\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree=5  #better results than with degree=6\n",
    "Xmap = mapFeature(X[:,0], X[:,1],degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Regularized Cost Function and Gradient\n",
    "\n",
    "Complete the code in *costFunctionReg* to return the cost function and gradient for regularized logistic regression. \n",
    "The cost function now has an additional penalty term that is controlled by the regularization hyper-parameter λ (also known as ridge regression).\n",
    "\n",
    "Note that you should not regularize the parameter  $\\theta_0$ \n",
    "\n",
    "$J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} [ -y^{(i)}log(h_{\\theta}(x^{(i)})) - (1 - y^{(i)})log(1 - (h_{\\theta}(x^{(i)}))] + \\frac{\\lambda}{2m} \\sum_{j=1}^n \\theta_j^2$\n",
    "\n",
    "$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})x_j^{(i)}$ for $j=0$\n",
    "\n",
    "$ \\frac{\\partial J(\\theta)}{\\partial \\theta_j} = \\frac{1}{m} \\sum_{i=1}^{m} (h_{\\theta}(x^{(i)}) - y^{(i)})x_j^{(i)} + \\frac{\\lambda}{m}\\theta_j$ for $j\\geq 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The same function as in Part 1\n",
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    return the sigmoid of z\n",
    "    \"\"\"\n",
    "    gz=?\n",
    "    \n",
    "    return gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " def costFunctionReg(X, y, theta, Lambda):\n",
    "    \"\"\"\n",
    "    Take in numpy array of  data X, labels y and theta, to return the regularized cost function and gradients\n",
    "    of the logistic regression classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    #number of training examples \n",
    "    m=?\n",
    "        \n",
    "    #vector of the model predictions for all training examples      \n",
    "    h = ?\n",
    "    \n",
    "    error = (-y * np.log(h)) - ((1-y)*np.log(1-h))\n",
    "    \n",
    "    #cost function without regularization term\n",
    "    cost = sum(error)/m\n",
    "    \n",
    "    #add regularization term to the cost function\n",
    "    regCost= cost + Lambda/(2*m) * sum(theta[1:]**2)\n",
    "    \n",
    "    #gradient of theta_0\n",
    "    grad_0= (1/m) * np.dot(X.transpose(),(h - y))[0]\n",
    "    \n",
    "    #vector of gradients of theta_j from j=1:n (adding the regularization term of the gradient)\n",
    "    grad = (1/m) * np.dot(X.transpose(),(h - y))[1:] + (Lambda/m)* theta[1:]\n",
    "       \n",
    "    # all gradients in a column vector shape\n",
    "    grad_all=np.append(grad_0,grad)\n",
    "    grad_all = grad_all.reshape((len(grad_all), 1))\n",
    "    \n",
    "    return regCost[0], grad_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize vector theta to 0 and call *costFunctionReg* function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize fitting parameters\n",
    "initial_theta = ?\n",
    "\n",
    "# Set regularization parameter lambda to 0\n",
    "Lambda = ?\n",
    "\n",
    "#Call CostFunctionReg and get the cost and gradients for initial_theta\n",
    "cost, grad= ?\n",
    "\n",
    "print(\"Cost for initial theta is\",round(cost,3) )  #ANSWER: Cost for initial theta is 0.693\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Complete gradientDescent function, which is the same as in Part 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,y,theta,alpha,num_iters,Lambda):\n",
    "    \"\"\"\n",
    "    Take in numpy array X, y and theta and update theta by taking num_iters gradient steps\n",
    "    with learning rate of alpha\n",
    "    \n",
    "    return theta and the list of the cost of theta during each iteration\n",
    "    \"\"\"\n",
    "    \n",
    "    J_history =[]\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        \n",
    "        #call CostFunctionReg \n",
    "        cost, grad = ?\n",
    "        \n",
    "        #update theta\n",
    "        theta = ?\n",
    "        \n",
    "        J_history.append(cost)\n",
    "    \n",
    "    return theta , J_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTS \n",
    "\n",
    "Make tests with different values of λ, keeping the learning rate alpha=0.5. Get similar plots as below. \n",
    "\n",
    "**λ = 0, iterat=10000, Train Accuracy:84.75 %**\n",
    "\n",
    "<img src=\"images/l0_a05_it10000_plot2.png\" style=\"width:250px;height:200px;\">\n",
    "<caption><center> **Fig. 3** : **Cost function evolution (λ = 0, 10000 iterations !!!)** </center></caption>\n",
    "\n",
    "\n",
    "<img src=\"images/l0_a05_it10000_plot1.png\" style=\"width:250px;height:200px;\">\n",
    "<caption><center> **Fig. 4** : **λ = 0,Train Accuracy: 84.75 %** </center></caption>\n",
    "\n",
    "**λ = 1, iterat=1000, Train Accuracy: 83.90 %**\n",
    "\n",
    "<img src=\"images/l1_a05_it1000_plot2.png\" style=\"width:250px;height:200px;\">\n",
    "<caption><center> **Fig. 5** : **Cost function evolution (λ = 1, 1000 iterations !!!)** </center></caption>\n",
    "\n",
    "\n",
    "<img src=\"images/l1_a05_it1000_plot1.png\" style=\"width:250px;height:200px;\">\n",
    "<caption><center> **Fig. 6** : **λ = 1,Train Accuracy: 83.90 %** </center></caption>\n",
    "\n",
    "\n",
    "**λ = 10, iterat=200, Train Accuracy: 71.2 %**\n",
    "\n",
    "<img src=\"images/L10_a05_it200_plot2.png\" style=\"width:250px;height:200px;\">\n",
    "<caption><center> **Fig. 7** : **Cost function evolution (λ = 10, 200 iterations !!!)** </center></caption>\n",
    "<img src=\"images/L10_a05_it200_plot1.png\" style=\"width:250px;height:200px;\">\n",
    "<caption><center> **Fig. 8** : **λ = 10,Train Accuracy: 71.20 %** </center></caption>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda =?\n",
    "alpha=?\n",
    "iterat=?\n",
    "initial_theta = ?\n",
    "theta , J_history = gradientDescent(Xmap,y,initial_theta,alpha,iterat,Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(J_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"$J(\\Theta)$\")\n",
    "plt.title(\"Cost function using Gradient Descent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the data and the decision boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapFeaturePlot(x1,x2,degree):\n",
    "    \"\"\"\n",
    "    take in numpy array of x1 and x2, return all polynomial terms up to the given degree\n",
    "    \"\"\"\n",
    "    out = np.ones(1)\n",
    "    for i in range(1,degree+1):\n",
    "        for j in range(i+1):\n",
    "            terms= (x1**(i-j) * x2**j)\n",
    "            out= np.hstack((out,terms))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Xmap[pos[:,0],1],Xmap[pos[:,0],2],c=\"r\",marker=\"+\",label=\"Admitted\")\n",
    "plt.scatter(Xmap[neg[:,0],1],Xmap[neg[:,0],2],c=\"b\",marker=\"x\",label=\"Not admitted\")\n",
    "\n",
    "# Plotting decision boundary\n",
    "\n",
    "u_vals = np.linspace(-1,1.5,50)\n",
    "v_vals= np.linspace(-1,1.5,50)\n",
    "z=np.zeros((len(u_vals),len(v_vals)))\n",
    "for i in range(len(u_vals)):\n",
    "    for j in range(len(v_vals)):\n",
    "        z[i,j] =mapFeaturePlot(u_vals[i],v_vals[j],degree) @ theta \n",
    "\n",
    "plt.contour(u_vals,v_vals,z.T,0)\n",
    "plt.xlabel(\"Exam 1 score\")\n",
    "plt.ylabel(\"Exam 2 score\")\n",
    "plt.legend(loc=0)\n",
    "plt.title('Lambda =?, alpha=0.5, iterat=?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model accuracy on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifierPredict(theta,X):\n",
    "    \"\"\"\n",
    "    take in numpy array of theta and X and predict the class \n",
    "    \"\"\"\n",
    "    h = np.dot(X, theta)\n",
    "    \n",
    "    return h>0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=classifierPredict(theta,Xmap)\n",
    "\n",
    "print(\"Train Accuracy:\", (sum(p==y)/len(y)*100)[0],\"%\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
