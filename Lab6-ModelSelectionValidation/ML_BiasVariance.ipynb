{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML 6- Regularized Linear Regression - Bias-Variance tradeoff\n",
    "\n",
    "**Objectives**: Implement Regularized Linear Regression algorithm and use it to study the bias-variance tradeoff.\n",
    "\n",
    "The task is to implement regularized linear regression to predict the amount of water owing out of a dam using the change of water level in a reservoir. You will examine the effects of bias versus variance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "The task is to implement regularized linear regression to predict the amount of water owing out of a dam using the change of water level in a reservoir. You will examine the effects of bias versus variance.\n",
    "\n",
    "File *ex5data1.mat* contains historical records on the change in the water level, x, and the amount of water owing out of the dam, y. The dataset is divided into the following parts:\n",
    "\n",
    "• Training set ( X, y) used to fit the model.\n",
    "\n",
    "• Cross validation set (Xval, yval) for determining the regularization parameter.\n",
    "\n",
    "• Test set (Xtest, ytest) for evaluating performance. These are examples which the model did not see during training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use loadmat to load the matlab file and extraact train, CV and test subsets. \n",
    "\n",
    "X= ?\n",
    "y= ?\n",
    "\n",
    "Xval=?\n",
    "yval=?\n",
    "\n",
    "Xtest= ?\n",
    "ytest= ?\n",
    "\n",
    "m = ?  # Number of training examples \n",
    "mtest = ? # Number of validation examples \n",
    "mval = ?  # Number of testing examples \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data\n",
    "\n",
    "Plot the training data and get a figure similar to Fig.1. \n",
    "\n",
    "<img src=\"images/f1.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> **Fig. 1** : **Training data** </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Fig.1. Use similar code from previous labs \n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularized Linear Regression Cost (Loss) Function\n",
    "\n",
    "Now, you will implement Linear Regression to fit a straight line to the data and plot the learning curves. \n",
    "\n",
    "The regularized Linear Regression Cost (Loss) Function is:\n",
    "\n",
    "$J(\\theta) = \\frac{1}{2m} (\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)})^2) + \\frac{\\lambda}{2m}(\\sum_{j=1}^n \\theta_j^2)$\n",
    "\n",
    "Recall that $\\lambda$ is the regularization parameter which helps preventing overfitting. The regularization term puts a penalty on the overall cost $J(\\theta)$. Note that you should not regularize $\\theta_0$  term. \n",
    "\n",
    "Complete the code in function *linearRegCostFunction* to calculate the Regularized Linear Regression Cost (Loss) function and its gradients with respect to thetas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearRegCostFunction(X, y, theta, Lambda):\n",
    "    \n",
    "    \"\"\"\n",
    "    Take in numpy array of  data X, labels y and theta, to return the regularized cost function and gradients\n",
    "    of the linear regression model.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Number of training examples \n",
    "    m = ?\n",
    "    \n",
    "    #linear regression model\n",
    "    h = ?\n",
    "    \n",
    "    cost = 1/(2*m) * np.sum((h - y)**2)\n",
    "    reg_cost = cost + Lambda/(2*m) * (np.sum(theta[1:]**2))\n",
    "    \n",
    "    # compute the gradient\n",
    "    grad_0= (1/m) * np.dot(X.transpose(),(h - y))[0]\n",
    "    grad = (1/m) * np.dot(X.transpose(),(h - y))[1:] + (Lambda/m)* theta[1:]\n",
    "       \n",
    "    #  make the complete gradient a column vector\n",
    "    grad_all=np.append(grad_0,grad)\n",
    "    grad_all = grad_all.reshape((len(grad_all), 1))\n",
    "    \n",
    "    return reg_cost, grad_all\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Linear Regression\n",
    "\n",
    "Once the cost function and the gradients are computed correctly, run *gradientDescent* to compute the optimal values of $\\theta$. \n",
    "\n",
    "Here, we set the regularization parameter $\\lambda$ = 0. Because the linear regression is trying to fit a 2D $\\theta$, regularization will not be much helpful for $\\theta$ of such low dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientDescent(X,y,theta,learn_rate,num_iters,Lambda):\n",
    "    \"\"\"\n",
    "    Take in numpy array X, y and theta and update theta by taking num_iters gradient steps\n",
    "    with learning rate of alpha\n",
    "    \n",
    "    return theta and the list of the cost of theta during each iteration\n",
    "    \"\"\"\n",
    "    \n",
    "    J_history =[]\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        cost, grad = linearRegCostFunction(X,y,theta,Lambda)\n",
    "        theta = theta - (learn_rate * grad)\n",
    "        J_history.append(cost)\n",
    "    \n",
    "    return theta, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add an extra columns of 1 to X (recall axis=0 are the rows, axis=1 are the columns)\n",
    "X_1 = \n",
    "\n",
    "Lambda = 0\n",
    "learn_rate=0.001\n",
    "#choose different number of iterations \n",
    "num_iter=300\n",
    "\n",
    "# inicialize all theta at 0. \n",
    "initial_theta = ?\n",
    "\n",
    "#compute the optimal theta\n",
    "theta, J_history = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Cost Function history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(J_history)\n",
    "#add labels\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the data and its linear model\n",
    "You should get a figure similar to Fig.2. This best fit line tells that the model is not a good fit to the data. \n",
    "\n",
    "<img src=\"images/f2.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> **Fig.2** : **Linear fit** </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#add the scatter plot of data (as above) \n",
    "?\n",
    "\n",
    "\n",
    "#Plot the best linear model\n",
    "x_fit=range(-50,40)\n",
    "y_fit=theta[0]+theta[1]*x_fit\n",
    "plt.plot(x_fit,y_fit,color=\"b\")\n",
    "\n",
    "plt.ylim(-5,40)\n",
    "plt.xlim(-50,40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias-variance and learning curves \n",
    "\n",
    "An important concept in Machine Learning is the **bias-variance** tradeoff. Models with high bias are not complex enough for the data and tend to underfit, while models with high variance overfit to the training data. \n",
    "\n",
    "Now, you will plot training and cross validation learning curves to diagnose bias-variance problems. \n",
    "To plot the learning curve, we need training and cross validation errors for different training set sizes. To obtain different training set sizes, *learningCurve* use different subsets of the original training set X. Specifically, for a training set size of i, the first i examples (i.e., X(0:i,:) and y(0:i)) are used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learningCurve(X, y, Xval, yval, learn_rate, num_iter, Lambda):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the train and cross validation set errors for a learning curve\n",
    "    \"\"\"\n",
    "   \n",
    "    m = ?  # Number of training examples \n",
    "    n=?  # number of features \n",
    "    mval = ?  # Number of validation examples \n",
    "\n",
    "    error_train, error_val = [],[]\n",
    "    \n",
    "    for i in range(1,m+1):\n",
    "        \n",
    "        # inicialize all theta at 0. \n",
    "        initial_theta= ?\n",
    "        Xtrain=X[0:i,:]\n",
    "        ytrain=y[0:i,:]\n",
    "        theta = gradientDescent( Xtrain, ytrain,initial_theta,learn_rate,num_iter,Lambda)[0]\n",
    "        \n",
    "        h_train = np.dot(Xtrain, theta)\n",
    "        h_val = np.dot(Xval, theta)\n",
    "        \n",
    "        error_train_i = 1/(2*m) * np.sum((h_train - ytrain)**2)\n",
    "        error_val_i = 1/(2*mval) * np.sum((h_val - yval)**2)\n",
    "        \n",
    "        error_train.append(error_train_i)\n",
    "        error_val.append(error_val_i)\n",
    "\n",
    "    return error_train, error_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add an extra columns of 1 to Xval\n",
    "Xval_1 = ?\n",
    "\n",
    "#Call learningCurve to compute E_train and E_validation\n",
    "error_train, error_val = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the learning curves\n",
    "\n",
    "Plot the learning curves as shown in Fig. 3. You can observe that both the training and the cross validation errors are high even when the number of training examples increases. This reflects a high bias problem of the model. \n",
    "\n",
    "<img src=\"images/f3.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> **Fig.3** : **Linear Regression learning curves** </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1,m+1),error_train)\n",
    "plt.plot(range(1,m+1), error_val,color=\"r\")\n",
    "plt.title(\"Learning Curve for Linear Regression\")\n",
    "plt.xlabel(\"Number of training examples\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.legend([\"Train\", \"Cross Validation\"])\n",
    "plt.ylim(-10,210)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Regression\n",
    "\n",
    "The linear model is too simple for this data and resulted in underfitting (high bias). Now, you will address this problem by adding more features using higher powers of the original feature (waterLevel), such as:\n",
    "\n",
    "<img src=\"images/f4.png\" style=\"width:450px;height:50px;\">\n",
    "<caption><center> **Fig.4**: **Polynomial Regression model** </center></caption>\n",
    "\n",
    "Keep in mind that even though we have polynomial terms in the model above, we are still solving a linear regression optimization problem. The polynomial terms are simply new features that we can use for linear regression. The function *polyFeatures* maps the original training set X of size mx1 into its higher powers. Specifically, when a training set X of size mx1 is passed into the function, the function should return a mxp matrix X_poly, where 1st column holds the original values of X, 2nd column holds the values of $X^2$,  3rd column holds the values of $X.^3$, and so on. \n",
    "\n",
    "Function *polyFeatures* is applied to the training, cross validation and test sets. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyFeatures(X, degree):\n",
    "    \"\"\"\n",
    "    Takes a data matrix X (size m x 1) and maps each example into its polynomial features where \n",
    "    X_poly(i, :) = [X(i) X(i).^2 X(i).^3 ...  X(i).^degree];\n",
    "    \"\"\"\n",
    "    m= ?  # Number of training examples \n",
    "    \n",
    "    for i in range(2,degree+1):\n",
    "        var=X[:,0]**i\n",
    "        X = np.append(X, var.reshape(m,1), axis=1)\n",
    "        \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Map X onto Polynomial features (call polyFeatures)\n",
    "degree=8\n",
    "X_poly = ?\n",
    "\n",
    "#Check the range of values for the polynomial features\n",
    "?\n",
    "\n",
    "#Normalize features with StandardScaler function of sklearn library\n",
    "sc_X=StandardScaler()\n",
    "X_poly_normalized=sc_X.fit_transform(X_poly)\n",
    "\n",
    "#Check the range of values for the normalized features \n",
    "?\n",
    "\n",
    "#Add an extra column of 1' to X_poly\n",
    "X_poly_normalized = ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Xtest onto polynomial features and normalize (the same way as X)\n",
    "\n",
    "X_poly_test_normalized=?\n",
    "\n",
    "#Add an extra column of 1' to X_poly_test\n",
    "X_poly_test_normalized = ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Xval onto polynomial features and normalize, add column of 1\n",
    "\n",
    "\n",
    "X_poly_val_normalized = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Polynomial Regression\n",
    "\n",
    "The training of the polynomial model uses the same linear regression cost function and gradient that you wrote for the earlier part of the assignment.\n",
    "\n",
    "Let assume a polynomial of degree 8. If we run the training directly on the data, it will not work well as the features would be badly scaled (e.g., an example with x = 40 will now have a feature x8 = 40^8 = 6.5x10^12). Therefore, before learning the parameters, the features of the training set are first normalized (function *featureNormalize*), storing the mean (mu) and the standard deviation (sigma) parameters. After training is over, you should see two plots (Figs. 5 and 6) generated for  = $\\lambda$=0.\n",
    "See that the polynomial fit is able to follow the data points very well (Fig.5) and therefore the training error is low (almost equal to zero in Fig.6). However, there is a gap between the training and cross validation errors (Fig.6), indicating a high variance problem due to the lack of regularization ( $\\lambda$=0). \n",
    "\n",
    "<img src=\"images/f5.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> **Fig.5**: **Polynomial Fit** </center></caption>\n",
    "\n",
    "\n",
    "<img src=\"images/f6.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> **Fig.6**: **Linear Regression learning curves** </center></caption>\n",
    "\n",
    "<img src=\"images/f7.png\" style=\"width:350px;height:250px;\">\n",
    "<caption><center> **Fig.7**: **Cost function trajectory** </center></caption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n is not the new number of features after adding polynomial terms\n",
    "n= ?\n",
    "\n",
    "Lambda=0\n",
    "num_iter=3000\n",
    "learn_rate=0.001\n",
    "\n",
    "# inicialize all theta at 0. \n",
    "init_theta= ?\n",
    "\n",
    "\n",
    "#call gradientDescent(X,y,theta,learn_rate,num_iters,Lambda)\n",
    "theta_poly, J_history_poly = gradientDescent(X_poly_normalized,y,init_theta,learn_rate,num_iter,Lambda) \n",
    "\n",
    "#Create Fig.7\n",
    "?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code to get Fig.5\n",
    "#add the scatter plot of data (as above) \n",
    "?\n",
    "\n",
    "xmin = np.min(X) - 15\n",
    "xmax=np.max(X) + 25\n",
    "\n",
    "x_value=np.linspace(xmin,xmax,2400)\n",
    "x_value=x_value.reshape(x_value.shape[0],1)\n",
    "\n",
    "# Map the X values and normalize\n",
    "x_value_poly = polyFeatures(x_value, degree)\n",
    "x_value_poly = sc_X.transform(x_value_poly)\n",
    "x_value_poly = np.append(np.ones((x_value_poly.shape[0],1)),x_value_poly, axis=1)\n",
    "y_value= np.dot(x_value_poly, theta_poly)\n",
    "plt.plot(x_value,y_value,\"--\",color=\"b\")\n",
    "plt.ylim(-5,160)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Call learningCurve to compute E_train and E_validation\n",
    "error_train, error_val = ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e7dd1907d32d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Code to get Fig.6\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0merror_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Train\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0merror_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Cross Validation\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Learning Curve for Linear Regression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "#Code to get Fig.6\n",
    "\n",
    "plt.plot(range(m),error_train,label=\"Train\")\n",
    "plt.plot(range(m),error_val,label=\"Cross Validation\",color=\"r\")\n",
    "plt.title(\"Learning Curve for Linear Regression\")\n",
    "plt.xlabel(\"Number of training examples\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.legend()\n",
    "plt.ylim(0,100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial regression with $\\lambda$ =  [0.001; 0.003; 0.01; 0.03; 0.1; 0.3; 1; 3; 10] ) \n",
    "\n",
    "Find the best $\\lambda$ from a grid search and compute the training, validation and test errors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validationCurve (X, y, Xval, yval, learn_rate, num_iter, Lambda_array):\n",
    "\n",
    "    \"\"\"\n",
    "    Returns the best lambda and the respective train and cross validation set errors\n",
    "    \"\"\"\n",
    "    m = ?  # Number of training examples \n",
    "    n=?  # number of features \n",
    "    mval = ?  # Number of validation examples \n",
    "    \n",
    "    error_train, error_val = [],[]\n",
    "    \n",
    "    for lam in Lambda_array:\n",
    "        theta_ini=np.zeros((n,1))\n",
    "        theta = gradientDescent( X, y,theta_ini,learn_rate,num_iter,lam)[0]\n",
    "        pred_train = np.dot(X,theta)\n",
    "        pred_val = np.dot(Xval,theta)\n",
    "        error_train_i = 1/(2*m) * np.sum((pred_train - y)**2)\n",
    "        error_val_i = 1/(2*mval) * np.sum((pred_val - yval)**2)\n",
    "        error_train.append(error_train_i)\n",
    "        error_val.append(error_val_i)\n",
    "    \n",
    "    ind = np.argmin(error_val)\n",
    "    best_lambda=Lambda_array[ind]\n",
    "        \n",
    "    return best_lambda, error_train, error_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda_array = [0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]\n",
    "\n",
    "best_lambda, error_train, error_val = validationCurve(X_poly_normalized, y, X_poly_val_normalized, yval, learn_rate, num_iter, Lambda_array)\n",
    "\n",
    "print(best_lambda)\n",
    "\n",
    "plt.plot(Lambda_array,error_train,label=\"Train\")\n",
    "plt.plot(Lambda_array,error_val,label=\"Cross Validation\",color=\"r\")\n",
    "plt.xlabel(\"Lambda\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.legend()\n",
    "\n",
    "#for the best_lambda computed above, print the training (Etrain), validation (Eval) and test (Etest) errors\n",
    "\n",
    "# inicialize all theta at 0. \n",
    "theta_ini=?\n",
    "theta_poly = gradientDescent(X_poly_normalized, y,theta_ini,learn_rate,num_iter,best_lambda)[0]\n",
    "\n",
    "pred = np.dot(X_poly_normalized,theta_poly)\n",
    "Etrain = 1/(2*m) * np.sum((pred - y)**2)\n",
    "\n",
    "pred_val = np.dot(X_poly_val_normalized,theta_poly)\n",
    "Eval = 1/(2*mval) * np.sum((pred_val - yval)**2)\n",
    "\n",
    "pred_test = np.dot(X_poly_test_normalized, theta_poly)\n",
    "Etest = 1/(2*mtest) * np.sum((pred_test - ytest)**2)\n",
    "\n",
    "print(Etrain)\n",
    "print(Eval)\n",
    "print(Etest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
